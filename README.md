---
title: Automated ML Pipeline with CI/CD
emoji: ğŸ¤–
colorFrom: gray
colorTo: red
sdk: docker
app_file: Dockerfile
pinned: false
license: mit
---

# Under Construction

# Notes

Raw dataset: https://www.kaggle.com/datasets/harlfoxem/housesalesprediction

py -3.10 -m venv .venv

.\\.venv\Scripts\activate

python -m pip install --upgrade pip

pip install -r requirements-dev.txt

python scripts/bootstrap.py

uvicorn app.main:app --reload

# Automated ML Pipeline with CI/CD

This repository contains a fully automated **Machine Learning pipeline** with **CI/CD capabilities**, designed for **house price prediction in King County, USA, 2015**. The project features reproducible model training, metric-based quality gates, versioned model packaging, and deployment-ready artifacts. It leverages **Python 3.10**, **scikit-learn** and **GitHub Actions** for automation, ensuring enterprise-grade reproducibility and governance.

The system is designed to run locally on venv python 3.10 or on Hugging Face Spaces, with minimal dependencies and a purposely simple front-end.

Hugging Face Space: [LeonardoMdSA / Automated ML Pipeline with CI/CD](https://huggingface.co/spaces/LeonardoMdSA/Automated-ML-Pipeline-with-CI-CD)

---

## Features

* **Automated Training & Evaluation:** Run training and evaluation pipelines with a single command.
* **Model Versioning:** Versioned models stored in a local registry (`models/registry`) with metadata.
* **Quality Gates:** Metric-based evaluation ensures only high-quality models are promoted.
* **Artifact Packaging:** Models packaged with metrics and metadata for reproducibility.
* **CI/CD Pipelines:** Fully automated using GitHub Actions for testing, evaluation, and deployment.
* **Web Interface:** Minimal FastAPI dashboard for predictions.

---

## Repository Structure (After running bootstrap.py)

```
Automated ML Pipeline with CI-CD/
â”œâ”€â”€ .github/
â”‚ â””â”€â”€ workflows/
â”‚ â”œâ”€â”€ deploy_hf.yml # Deploy inference app to Hugging Face Spaces
â”‚ â””â”€â”€ ml_pipeline.yml # CI pipeline: train, evaluate, gate, package
â”œâ”€â”€ .vscode/
â”‚ â””â”€â”€ settings.json # VS Code workspace settings
â”œâ”€â”€ app/ # Inference service (FastAPI)
â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â””â”€â”€ routes.py # Prediction API routes
â”‚ â”œâ”€â”€ core/
â”‚ â”‚ â”œâ”€â”€ config.py # App configuration
â”‚ â”‚ â””â”€â”€ logging.py # Structured logging setup
â”‚ â”œâ”€â”€ inference/
â”‚ â”‚ â””â”€â”€ predictor.py # Loads *packaged* model and runs inference
â”‚ â”œâ”€â”€ schemas/
â”‚ â”‚ â””â”€â”€ request_response.py # Pydantic request/response schemas
â”‚ â”œâ”€â”€ static/
â”‚ â”‚ â””â”€â”€ styles.css # Frontend styles
â”‚ â”œâ”€â”€ templates/
â”‚ â”‚ â””â”€â”€ index.html # Minimal HTML frontend
â”‚ â””â”€â”€ main.py # FastAPI application entrypoint
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â”‚ â””â”€â”€ kc_house_data.csv # Raw dataset
â”‚ â””â”€â”€ processed/
â”‚ â””â”€â”€ train_test.npz # Train/test split artifacts
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ baseline/
â”‚ â”‚ â””â”€â”€ metrics.json # Baseline model metrics
â”‚ â”œâ”€â”€ packaged/ # Production-ready model artifact
â”‚ â”‚ â”œâ”€â”€ model.pkl # Serialized best model
â”‚ â”‚ â”œâ”€â”€ metrics.json # Metrics of packaged model
â”‚ â”‚ â””â”€â”€ packaged.json # Packaging metadata
â”‚ â””â”€â”€ registry/ # Filesystem-based model registry
â”‚ â”œâ”€â”€ model_v001/
â”‚ â”‚ â”œâ”€â”€ model.pkl
â”‚ â”‚ â””â”€â”€ metadata.json
â”‚ â”œâ”€â”€ model_v002/
â”‚ â”‚ â”œâ”€â”€ model.pkl
â”‚ â”‚ â””â”€â”€ metadata.json
â”‚ â”œâ”€â”€ model_v003/
â”‚ â”‚ â”œâ”€â”€ model.pkl
â”‚ â”‚ â””â”€â”€ metadata.json
â”‚ â””â”€â”€ latest.json # Pointer to most recent trained model
â”œâ”€â”€ reports/
â”‚ â”œâ”€â”€ evaluations/ # Per-run evaluation reports
â”‚ â”‚ â”œâ”€â”€ model_v001_run*.json
â”‚ â”‚ â”œâ”€â”€ model_v002_run*.json
â”‚ â”‚ â””â”€â”€ model_v003_run*.json
â”‚ â””â”€â”€ comparison.json # Model comparison results
â”œâ”€â”€ scripts/ # Pipeline execution scripts
â”‚ â”œâ”€â”€ bootstrap.py # End-to-end local bootstrap
â”‚ â”œâ”€â”€ train.py # Deterministic model training
â”‚ â”œâ”€â”€ evaluate.py # Model evaluation
â”‚ â”œâ”€â”€ compare.py # Compare candidate vs baseline
â”‚ â”œâ”€â”€ metric_gate.py # Quality gate enforcement
â”‚ â”œâ”€â”€ package_model.py # Package best model for inference
â”‚ â”œâ”€â”€ versioning.py # Model version increment logic
â”‚ â”œâ”€â”€ config.py # Pipeline configuration
â”‚ â””â”€â”€ __init__.py
â”œâ”€â”€ tests/
â”‚ â”œâ”€â”€ integration/ # End-to-end and CI-like tests
â”‚ â”‚ â”œâ”€â”€ test_api_predict.py
â”‚ â”‚ â”œâ”€â”€ test_ci_like_flow.py
â”‚ â”‚ â”œâ”€â”€ test_gate_blocks_regression.py
â”‚ â”‚ â”œâ”€â”€ test_model_promotion.py
â”‚ â”‚ â””â”€â”€ test_train_evaluate_pipeline.py
â”‚ â”œâ”€â”€ unit/ # Deterministic unit tests
â”‚ â”‚ â”œâ”€â”€ test_compare_gate_logic.py
â”‚ â”‚ â”œâ”€â”€ test_compare_self_comparison_guard.py
â”‚ â”‚ â”œâ”€â”€ test_data_schema.py
â”‚ â”‚ â”œâ”€â”€ test_evaluate_deterministic.py
â”‚ â”‚ â”œâ”€â”€ test_metrics_computation.py
â”‚ â”‚ â”œâ”€â”€ test_metric_gate.py
â”‚ â”‚ â”œâ”€â”€ test_registry_metadata.py
â”‚ â”‚ â”œâ”€â”€ test_train_deterministic.py
â”‚ â”‚ â”œâ”€â”€ test_train_outputs.py
â”‚ â”‚ â””â”€â”€ test_version_increment.py
â”‚ â”œâ”€â”€ conftest.py
â”‚ â””â”€â”€ __init__.py
â”œâ”€â”€ Dockerfile # Container ready for Hugging Face Spaces
â”œâ”€â”€ pytest.ini # Pytest configuration
â”œâ”€â”€ requirements.txt # Runtime dependencies
â””â”€â”€ repo_structure.py # Utility to print repo tree
```

---

## Installation

1. Clone the repository:

```bash
git clone https://github.com/LeonardoMdSACode/Automated-ML-Pipeline-CI-CD-Clean.git
cd Automated-ML-Pipeline-CI-CD-Clean
```

2. Create a virtual environment and install dependencies:

```bash
py -3.10 -m venv .venv
source .venv/bin/activate  # Linux/Mac
.\.venv\Scripts\activate   # Windows
pip install -r requirements.txt
```

---

## Usage

* **Train a model:** `python scripts/train.py`
* **Evaluate a model:** `python scripts/evaluate.py --model models/packaged/model.pkl`
* **Compare models and apply gates:** `python scripts/compare.py`
* **Package model for deployment:** `python scripts/package_model.py`

#### Or just run: `python scripts/bootstrap.py` instead.

1. Run the FastAPI app locally:

```bash
uvicorn app.main:app --reload
```

* **Check API predictions:** Open `http://127.0.0.1:8000` in your browser

---

## Testing

1. Run all tests with pytest:

   ```bash
   pytest -v
   ```

2. Tests will run regardless during CI at github actions.

---

## Logical Architecture Overview

This project is organized as a **strictly layered, contract-driven ML system**. Each layer has a single responsibility and clear boundaries. Only gated, packaged artifacts are allowed to reach production.

---

### 1. Data Layer
**Responsibility:** Provide deterministic, version-stable inputs.

- `data/raw/` â€“ immutable source dataset  
- `data/processed/` â€“ canonical train/test split (`train_test.npz`)
- All training and evaluation consume processed data only

---

### 2. Training Layer
**Responsibility:** Produce reproducible candidate models.

- `scripts/train.py` â€“ deterministic model training
- `scripts/versioning.py` â€“ semantic model versioning (`model_vXXX`)
- Outputs candidate models to `models/registry/`

---

### 3. Evaluation Layer
**Responsibility:** Measure model performance in isolation.

- `scripts/evaluate.py` â€“ computes metrics on test data
- `reports/evaluations/` â€“ immutable evaluation artifacts per run
- No comparison or promotion logic at this stage

---

### 4. Comparison & Quality Gate Layer
**Responsibility:** Enforce model quality and prevent regressions.

- `scripts/compare.py` â€“ candidate vs baseline / previous best
- `scripts/metric_gate.py` â€“ hard metric thresholds
- `models/baseline/` â€“ reference metrics
- Any regression blocks promotion

---

### 5. Model Registry Layer
**Responsibility:** Preserve all approved historical models.

- `models/registry/model_vXXX/`
  - `model.pkl`
  - `metadata.json`
- `models/registry/latest.json` â€“ pointer to latest approved model
- Append-only, never used directly by inference

---

### 6. Packaging Layer (Production Boundary)
**Responsibility:** Create the only deployable artifact.

- `scripts/package_model.py` â€“ freezes the approved model
- `models/packaged/`
  - `model.pkl`
  - `metrics.json`
  - `packaged.json`
- Model used in production

---

### 7. Inference Layer
**Responsibility:** Serve predictions from a frozen artifact.

- `app/inference/predictor.py`
- Loads `models/packaged/model.pkl`
- Stateless and read-only

---

### 8. API Layer
**Responsibility:** Expose inference via a stable contract.

- `app/api/routes.py` â€“ `/api/predict`
- `app/schemas/request_response.py` â€“ strict validation
- No ML logic in routes

---

### 9. Application Core Layer
**Responsibility:** Cross-cutting concerns.

- `app/core/config.py` â€“ configuration
- `app/core/logging.py` â€“ structured logging
- No business or ML logic

---

### 10. Presentation Layer (UI)
**Responsibility:** Human interaction only.

- `app/templates/index.html`
- `app/static/styles.css`
- Communicates exclusively via the API

---

### 11. Testing Layer
**Responsibility:** Enforce contracts and prevent regressions.

- `tests/unit/` â€“ deterministic behavior, metrics, versioning
- `tests/integration/` â€“ end-to-end pipeline and CI-like flows
- Every layer boundary is tested

---

### 12. CI/CD Orchestration Layer
**Responsibility:** Automation and governance.

- `.github/workflows/ml_pipeline.yml` â€“ train â†’ evaluate â†’ gate â†’ package
- `.github/workflows/deploy_hf.yml` â€“ deploy packaged model
- No manual promotion or deployment

---
## Technology Stack

* Python 3.10
* FastAPI
* Uvicorn
* scikit-learn
* GitHub Actions (CI/CD)
* Pydantic 2.12.5
* Jinja2
* Pandas / NumPy
* Joblib (for model serialization)
* Docker (for containerized deployment)
* Hugging Face Spaces (deployment)

This project demonstrates a **reproducible, fully automated ML pipeline** with **enterprise-grade CI/CD practices**, suitable for real-world deployment and model governance.

---

## Recommendations & Important Notes

### Model Usage (Critical)
- **Inference always uses the packaged model** located in `models/packaged/model.pkl`.
- The **model registry (`models/registry/`) is append-only and NOT used by the API**.
- Never point inference to `latest.json` or directly to registry versions.
- The packaged model represents the **best model after metric gating**.

---

### CI/CD Guarantees
- Model promotion is **fully automated** and governed by metric gates.
- Any performance regression (e.g., RMSE increase) **blocks packaging and deployment**.
- Manual model promotion is intentionally unsupported.

---

### Determinism & Reproducibility
- Training, evaluation, and comparison are **deterministic by design**.
- Fixed random seeds and immutable processed datasets are enforced.
- Re-running the pipeline with identical inputs produces identical artifacts.

---

### Tests Are First-Class Citizens
- Unit tests validate:
  - Metric correctness
  - Version increments
  - Registry metadata integrity
  - Gate logic
- Integration tests simulate:
  - Full CI-like flows
  - Model regression blocking
  - API inference against packaged models
- Removing or weakening tests undermines pipeline guarantees.

---

### Registry vs Packaged Model (Design Intent)
- `models/registry/` is for **traceability and auditability**.
- `models/packaged/` is the **single production boundary**.
- This separation prevents accidental deployment of unvalidated models.

---

### DVC Removal (Intentional)
- DVC was removed to keep the pipeline:
  - Free-tier compatible
  - Self-contained
  - Fully reproducible via filesystem artifacts
- Data versioning is handled via **immutable processed datasets** and CI enforcement.

---

### Frontend Scope
- The UI is intentionally minimal and **not a data science tool**.
- Its purpose is:
  - Demonstrate inference
  - Display prediction + model version
- All business logic lives in the backend.

---

### Deployment Expectations
- The service is designed to run:
  - Locally via Docker
  - In CI
  - On Hugging Face Spaces (CPU, free tier)
- No cloud credentials or paid services are required.

---

### Extensibility Notes
- Drift detection, monitoring, and alerting can be added **without modifying**:
  - Training
  - Evaluation
  - Packaging logic
- The architecture supports post-deployment observability as a separate layer.

---

### Anti-Patterns (Do Not Do This)
- âŒ Loading models directly from `models/registry/`
- âŒ Skipping metric gates
- âŒ Manually copying models into `packaged/`
- âŒ Training inside the API service
- âŒ Treating `latest.json` as production truth

---

### Intended Audience
- This project is designed to demonstrate:
  - Production-grade ML engineering
  - CI-governed model promotion
  - Deterministic ML pipelines
- It is **not** a notebook-centric or experimentation-only project.

---

## References / Documentation

### Core Technologies
- **FastAPI** â€“ High-performance Python web framework for ML inference APIs  
  https://fastapi.tiangolo.com/

- **Uvicorn** â€“ ASGI server used for running FastAPI services  
  https://www.uvicorn.org/

- **scikit-learn** â€“ Model training, evaluation, and serialization  
  https://scikit-learn.org/stable/

- **NumPy** â€“ Numerical computing and deterministic data handling  
  https://numpy.org/doc/

---

### Testing & Quality
- **pytest** â€“ Unit and integration testing framework  
  https://docs.pytest.org/

- **pytest-cov** â€“ Test coverage reporting  
  https://pytest-cov.readthedocs.io/

---

### MLOps & Engineering Practices
- **ML Test Score (Google)** â€“ Testing levels for ML systems  
  https://research.google/pubs/pub46555/

- **Hidden Technical Debt in ML Systems** â€“ Foundational ML engineering paper  
  https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf

- **Effective Model Versioning** â€“ Practical guidance on model registries and promotion  
  https://martinfowler.com/articles/machine-learning-model-versioning.html

---

### CI/CD & Automation
- **GitHub Actions** â€“ CI/CD workflows for training, testing, and deployment  
  https://docs.github.com/en/actions

- **Semantic Versioning** â€“ Versioning principles applied to model artifacts  
  https://semver.org/

---

### Deployment
- **Docker** â€“ Containerized deployment for reproducible inference services  
  https://docs.docker.com/

- **Hugging Face Spaces** â€“ Free-tier deployment target for ML applications  
  https://huggingface.co/docs/hub/spaces

---

### Dataset
- **King County House Sales Dataset (2014â€“2015)**  
  https://www.kaggle.com/datasets/harlfoxem/housesalesprediction

---

### Design Philosophy
- **Twelve-Factor App** â€“ Principles for production-ready services  
  https://12factor.net/

- **Separation of Concerns** â€“ Architectural principle applied to training, evaluation, and inference  
  https://en.wikipedia.org/wiki/Separation_of_concerns

---
## Contact / Author

* Hugging Face: [https://huggingface.co/LeonardoMdSA](https://huggingface.co/LeonardoMdSA)
* GitHub: [https://github.com/LeonardoMdSACode](https://github.com/LeonardoMdSACode)

---

## MIT License

This project is licensed under the MIT License. See the LICENSE file for details.

[![License](https://img.shields.io/badge/license-MIT-blue)](LICENSE)
